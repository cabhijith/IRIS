{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from fastai import *\n",
    "from numpy.random import seed\n",
    "seed(0)\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Masking, Dense, concatenate, multiply, subtract, Dropout, Embedding, LSTM, GRU, Bidirectional, GlobalMaxPooling1D, Input, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from my_layers import SelfAttLayer, weightedAccCallback\n",
    "from score import score_submission, print_confusion_matrix, report_score\n",
    "from eventregistry import *\n",
    "from feature_engineering import polarity_features, refuting_features, word_overlap_features, hand_features\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import unidecode\n",
    "from keras.preprocessing import sequence\n",
    "import re\n",
    "from keras.models import load_model\n",
    "\n",
    "abbr_list = [\"n't\",\"'d\",\"'ll\",\"'s\",\"'m\",\"'ve\",\"'re\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_classify = load_learner(Path('Fake-News'), 'final-6Ji.pkl') #Load the weights of the linguistics model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the stance detection model \n",
    "\n",
    "def closest_word(originalWord, embeddings):\n",
    "    words = list(embeddings.keys())\n",
    "    currentClosest = words[0]\n",
    "    for word in words:\n",
    "        if jellyfish.jaro_winkler(originalWord, word) > jellyfish.jaro_winkler(originalWord, currentClosest):\n",
    "            currentClosest = word\n",
    "    print(\"Closest word to \" + originalWord +\" is \" + currentClosest)\n",
    "    return embeddings[currentClosest]\n",
    "\n",
    "def remove_parenthesis(sent):\n",
    "    return ' '.join(sent.replace('(', ' ').replace(')', ' ').replace('.', '').split()).lower()\n",
    "\n",
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
    "def clean_fnc(s):\n",
    "    s = unidecode.unidecode(s) # for correct tokenization\n",
    "    tokens = word_tokenize(s)\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if tok not in abbr_list:\n",
    "            tokens[i] = clean(tok)\n",
    "    return ' '.join(list(filter(lambda x: x != '', tokens))).lower()\n",
    "\n",
    "\n",
    "#/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "# imports #\n",
    "from numpy.random import seed\n",
    "seed(0)\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Masking, Dense, concatenate, multiply, subtract, Dropout, Embedding, LSTM, GRU, Bidirectional, GlobalMaxPooling1D, Input, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from my_layers import SelfAttLayer, weightedAccCallback\n",
    "from score import score_submission, print_confusion_matrix, report_score\n",
    "\n",
    "# Some hyperparameters #\n",
    "hidden_units = 300\n",
    "max_seq_len = 50\n",
    "max_seqs = 30\n",
    "########################\n",
    "\n",
    "print(\"Opening features\")\n",
    "with open('Data_Fake/features.pkl', 'rb') as inpFeat:\n",
    "    overlapFeatures_fnc = pickle.load(inpFeat)\n",
    "    refutingFeatures_fnc = pickle.load(inpFeat)\n",
    "    polarityFeatures_fnc = pickle.load(inpFeat)\n",
    "    handFeatures_fnc = pickle.load(inpFeat)\n",
    "    overlapFeatures_fnc_test = pickle.load(inpFeat)\n",
    "    refutingFeatures_fnc_test = pickle.load(inpFeat)\n",
    "    polarityFeatures_fnc_test = pickle.load(inpFeat)\n",
    "    handFeatures_fnc_test = pickle.load(inpFeat)\n",
    "    overlapFeatures_nli = pickle.load(inpFeat)\n",
    "    refutingFeatures_nli = pickle.load(inpFeat)\n",
    "    polarityFeatures_nli = pickle.load(inpFeat)\n",
    "    handFeatures_nli = pickle.load(inpFeat)\n",
    "    overlapFeatures_nli_test = pickle.load(inpFeat)\n",
    "    refutingFeatures_nli_test = pickle.load(inpFeat)\n",
    "    polarityFeatures_nli_test = pickle.load(inpFeat)\n",
    "    handFeatures_nli_test = pickle.load(inpFeat)\n",
    "    overlapFeatures_matched_test = pickle.load(inpFeat)\n",
    "    refutingFeatures_matched_test = pickle.load(inpFeat)\n",
    "    polarityFeatures_matched_test = pickle.load(inpFeat)\n",
    "    handFeatures_matched_test = pickle.load(inpFeat)\n",
    "    overlapFeatures_mismatched_test = pickle.load(inpFeat)\n",
    "    refutingFeatures_mismatched_test = pickle.load(inpFeat)\n",
    "    polarityFeatures_mismatched_test = pickle.load(inpFeat)\n",
    "    handFeatures_mismatched_test = pickle.load(inpFeat)\n",
    "    overlapFeatures_fnc_two = pickle.load(inpFeat)\n",
    "    refutingFeatures_fnc_two = pickle.load(inpFeat)\n",
    "    polarityFeatures_fnc_two = pickle.load(inpFeat)\n",
    "    handFeatures_fnc_two = pickle.load(inpFeat)\n",
    "    overlapFeatures_fnc_two_test = pickle.load(inpFeat)\n",
    "    refutingFeatures_fnc_two_test = pickle.load(inpFeat)\n",
    "    polarityFeatures_fnc_two_test = pickle.load(inpFeat)\n",
    "    handFeatures_fnc_two_test = pickle.load(inpFeat)\n",
    "    bleu_nli = pickle.load(inpFeat)\n",
    "    bleu_nli_test = pickle.load(inpFeat)\n",
    "    bleu_matched = pickle.load(inpFeat)\n",
    "    bleu_mismatched = pickle.load(inpFeat)\n",
    "    rouge_nli = pickle.load(inpFeat)\n",
    "    rouge_nli_test = pickle.load(inpFeat)\n",
    "    rouge_matched = pickle.load(inpFeat)\n",
    "    rouge_mismatched = pickle.load(inpFeat)\n",
    "    bleu_fnc = pickle.load(inpFeat)\n",
    "    bleu_fnc_test = pickle.load(inpFeat)\n",
    "    bleu_two_sentences = pickle.load(inpFeat)\n",
    "    bleu_two_sentences_test = pickle.load(inpFeat)\n",
    "    rouge_fnc = pickle.load(inpFeat)\n",
    "    rouge_fnc_test = pickle.load(inpFeat)\n",
    "    rouge_two_sentences = pickle.load(inpFeat)\n",
    "    rouge_two_sentences_test = pickle.load(inpFeat)\n",
    "\n",
    "del overlapFeatures_nli, refutingFeatures_nli, polarityFeatures_nli, handFeatures_nli, overlapFeatures_nli_test , refutingFeatures_nli_test, \\\n",
    "    polarityFeatures_nli_test, handFeatures_nli_test, overlapFeatures_matched_test, refutingFeatures_matched_test, polarityFeatures_matched_test, \\\n",
    "    handFeatures_matched_test, overlapFeatures_mismatched_test, refutingFeatures_mismatched_test, polarityFeatures_mismatched_test, \\\n",
    "    handFeatures_mismatched_test, bleu_nli, bleu_nli_test, bleu_matched, bleu_mismatched, rouge_nli, rouge_nli_test, rouge_matched, rouge_mismatched\n",
    "\n",
    "print(\"Opening variables\")\n",
    "with open('Data_Fake/variables.pkl', 'rb') as inp:\n",
    "    embedding_weights = pickle.load(inp)\n",
    "    X1 = pickle.load(inp)\n",
    "    X2 = pickle.load(inp)\n",
    "    Y = pickle.load(inp)\n",
    "    X1_test = pickle.load(inp)\n",
    "    X2_test = pickle.load(inp)\n",
    "    Y_test = pickle.load(inp)\n",
    "    X1_nli = pickle.load(inp)\n",
    "    X2_nli = pickle.load(inp)\n",
    "    Y_nli = pickle.load(inp)\n",
    "    X1_test_nli = pickle.load(inp)\n",
    "    X2_test_nli = pickle.load(inp)\n",
    "    Y_test_nli = pickle.load(inp)\n",
    "    X1_test_matched = pickle.load(inp)\n",
    "    X2_test_matched = pickle.load(inp)\n",
    "    Y_test_matched = pickle.load(inp)\n",
    "    X1_test_mismatched = pickle.load(inp)\n",
    "    X2_test_mismatched = pickle.load(inp)\n",
    "    Y_test_mismatched = pickle.load(inp)\n",
    "    X2_two_sentences = pickle.load(inp)\n",
    "    X2_test_two_sentences = pickle.load(inp)\n",
    "    tokenizer = pickle.load(inp)\n",
    "\n",
    "    \n",
    "del X1_nli, X2_nli, Y_nli, X1_test_nli, X2_test_nli, Y_test_nli, X1_test_matched, X2_test_matched, Y_test_matched, X1_test_mismatched, \\\n",
    "    X2_test_mismatched, Y_test_mismatched\n",
    "\n",
    "print(\"Opening similarities\")\n",
    "with open('Data_Fake/similarity.pkl', 'rb') as inpSim:\n",
    "    cosFeatures = pickle.load(inpSim)\n",
    "    cosFeatures_test = pickle.load(inpSim)\n",
    "    cosFeatures_nli = pickle.load(inpSim)\n",
    "    cosFeatures_nli_test = pickle.load(inpSim)\n",
    "    cosFeatures_matched = pickle.load(inpSim)\n",
    "    cosFeatures_mismatched = pickle.load(inpSim)\n",
    "\n",
    "cosFeatures = np.array(cosFeatures)\n",
    "cosFeatures_test = np.array(cosFeatures_test)\n",
    "\n",
    "cosFeatures_fnc = []\n",
    "cosFeatures_two = []\n",
    "for feat in cosFeatures:\n",
    "    cosFeatures_fnc += [feat[0]]\n",
    "    cosFeatures_two += [feat[1]]\n",
    "cosFeatures_fnc = np.array(cosFeatures_fnc)\n",
    "cosFeatures_two = np.array(cosFeatures_two)\n",
    "\n",
    "cosFeatures_fnc_test = []\n",
    "cosFeatures_two_test = []\n",
    "for feat in cosFeatures_test:\n",
    "    cosFeatures_fnc_test += [feat[0]]\n",
    "    cosFeatures_two_test += [feat[1]]\n",
    "cosFeatures_fnc_test = np.array(cosFeatures_fnc_test)\n",
    "cosFeatures_two_test = np.array(cosFeatures_two_test)\n",
    "\n",
    "del cosFeatures_nli, cosFeatures_nli_test, cosFeatures_matched, cosFeatures_mismatched\n",
    "\n",
    "with open(\"Data_Fake/cider_fnc.pkl\", \"rb\") as ciderFile:\n",
    "    cider_fnc_train = pickle.load(ciderFile, encoding='latin1')\n",
    "    cider_fnc_test = pickle.load(ciderFile, encoding='latin1')\n",
    "    cider_two_train = pickle.load(ciderFile, encoding='latin1')\n",
    "    cider_two_test = pickle.load(ciderFile, encoding='latin1')\n",
    "\n",
    "import pickle as cPickle\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.basic.pkl\", \"rb\") as countsTrain:\n",
    "    names = cPickle.load(countsTrain)\n",
    "    talos_counts_train = cPickle.load(countsTrain, encoding='latin1')\n",
    "print('Done')\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.basic.pkl\", \"rb\") as countsTest:\n",
    "    names = cPickle.load(countsTest)\n",
    "    talos_counts_test = cPickle.load(countsTest, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.sim.tfidf.pkl\", \"rb\") as tfidfSim_train:\n",
    "    talos_tfidfsim_train = cPickle.load(tfidfSim_train, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.sim.tfidf.pkl\", \"rb\") as tfidfSim_test:\n",
    "    talos_tfidfsim_test = cPickle.load(tfidfSim_test, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.headline.svd.pkl\", \"rb\") as svdHealine_train:\n",
    "    talos_svdHeadline_train = cPickle.load(svdHealine_train, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.headline.svd.pkl\", \"rb\") as svdHealine_test:\n",
    "    talos_svdHeadline_test = cPickle.load(svdHealine_test, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.body.svd.pkl\", \"rb\") as svdBody_train:\n",
    "    talos_svdBody_train = cPickle.load(svdBody_train, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.body.svd.pkl\", \"rb\") as svdBody_test:\n",
    "    talos_svdBody_test = cPickle.load(svdBody_test, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.sim.svd.pkl\", \"rb\") as svdSim_train:\n",
    "    talos_svdsim_train = cPickle.load(svdSim_train, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.sim.svd.pkl\", \"rb\") as svdSim_test:\n",
    "    talos_svdsim_test = cPickle.load(svdSim_test, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.headline.word2vec.pkl\", \"rb\") as w2vHealine_train:\n",
    "    talos_w2vHeadline_train = cPickle.load(w2vHealine_train, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.headline.word2vec.pkl\", \"rb\") as w2vHealine_test:\n",
    "    talos_w2vHeadline_test = cPickle.load(w2vHealine_test, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.body.word2vec.pkl\", \"rb\") as w2vBody_train:\n",
    "    talos_w2vBody_train = cPickle.load(w2vBody_train, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.body.word2vec.pkl\", \"rb\") as w2vBody_test:\n",
    "    talos_w2vBody_test = cPickle.load(w2vBody_test, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.sim.word2vec.pkl\", \"rb\") as w2vSim_train:\n",
    "    talos_w2vsim_train = cPickle.load(w2vSim_train, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.sim.word2vec.pkl\", \"rb\") as w2vSim_test:\n",
    "    talos_w2vsim_test = cPickle.load(w2vSim_test, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.headline.senti.pkl\", \"rb\") as sentiHealine_train:\n",
    "    talos_sentiHeadline_train = cPickle.load(sentiHealine_train, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.headline.senti.pkl\", \"rb\") as sentiHealine_test:\n",
    "    talos_sentiHeadline_test = cPickle.load(sentiHealine_test, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/train.body.senti.pkl\", \"rb\") as sentiBody_train:\n",
    "    talos_sentiBody_train = cPickle.load(sentiBody_train, encoding='latin1')\n",
    "\n",
    "with open(\"talos-fnc-1-py3/tree_model/test.body.senti.pkl\", \"rb\") as sentiBody_test:\n",
    "    talos_sentiBody_test = cPickle.load(sentiBody_test, encoding='latin1')\n",
    "\n",
    "########################## Definir o modelo ##################################### \n",
    "\n",
    "#Define some model layers #\n",
    "print('Done ! Done ! Done !')\n",
    "early_stop = EarlyStopping(monitor='loss', patience=2, verbose=1, restore_best_weights=True)\n",
    "weightedAccuracy = weightedAccCallback(X1_test, X2_test, Y_test, overlapFeatures_fnc_test, refutingFeatures_fnc_test, polarityFeatures_fnc_test, handFeatures_fnc_test,  \\\n",
    "                                       cosFeatures_fnc_test,cosFeatures_two_test, bleu_fnc_test, rouge_fnc_test,cider_fnc_test, X2_test_two_sentences, overlapFeatures_fnc_two_test, \\\n",
    "                                       refutingFeatures_fnc_two_test, polarityFeatures_fnc_two_test, handFeatures_fnc_two_test, \\\n",
    "                                       bleu_two_sentences_test, rouge_two_sentences_test, cider_two_test\n",
    "                                       , talos_counts_test, talos_tfidfsim_test, talos_svdHeadline_test, \\\n",
    "                                       talos_svdBody_test, talos_svdsim_test,talos_w2vHeadline_test, talos_w2vBody_test, talos_w2vsim_test, talos_sentiHeadline_test, talos_sentiBody_test)\n",
    "                                     \n",
    "\n",
    "embedding_layer = Embedding( embedding_weights.shape[0], embedding_weights.shape[1], input_length=max_seq_len, weights=[embedding_weights], trainable=False )\n",
    "lstm1 = LSTM(hidden_units, implementation=2, return_sequences=True, name='lstm1' )\n",
    "lstm1 = Bidirectional(lstm1, name='bilstm1')\n",
    "right_branch_lstm1 = LSTM(hidden_units, implementation=2, return_sequences=True )\n",
    "right_branch_lstm1 = Bidirectional(right_branch_lstm1)\n",
    "\n",
    "#####################################\n",
    "\n",
    "# Define the inputs for the model #\n",
    "\n",
    "input_headline = Input(shape=(max_seq_len,))\n",
    "input_two = Input(shape=(max_seq_len,))\n",
    "input_body = Input(shape=(max_seqs, max_seq_len,))\n",
    "input_overlap = Input(shape=(1,))\n",
    "input_overlap_two = Input(shape=(1,))\n",
    "input_refuting = Input(shape=(15,))\n",
    "input_refuting_two = Input(shape=(15,))\n",
    "input_polarity = Input(shape=(2,))\n",
    "input_polarity_two = Input(shape=(2,))\n",
    "input_hand = Input(shape=(26,))\n",
    "input_hand_two = Input(shape=(26,))\n",
    "input_sim = Input(shape=(1,))\n",
    "input_sim_two = Input(shape=(1,))\n",
    "input_bleu = Input(shape=(1,))\n",
    "input_bleu_two = Input(shape=(1,))\n",
    "input_rouge = Input(shape=(3,))\n",
    "input_rouge_two = Input(shape=(3,))\n",
    "input_cider = Input(shape=(1,))\n",
    "input_cider_two = Input(shape=(1,))\n",
    "\n",
    "input_talos_count = Input(shape=(41,))\n",
    "input_talos_tfidfsim = Input(shape=(1,))\n",
    "input_talos_headline_svd = Input(shape=(50,))\n",
    "input_talos_body_svd = Input(shape=(50,))\n",
    "input_talos_svdsim = Input(shape=(1,))\n",
    "input_talos_headline_w2v = Input(shape=(300,))\n",
    "input_talos_body_w2v = Input(shape=(300,))\n",
    "input_talos_w2vsim = Input(shape=(1,))\n",
    "input_talos_headline_senti = Input(shape=(4,))\n",
    "input_talos_body_senti = Input(shape=(4,))\n",
    "\n",
    "\n",
    "###############################\n",
    "\n",
    "# Define the sentence encoder #\n",
    "\n",
    "mask = Masking(mask_value=0, input_shape=(max_seq_len,))(input_headline)\n",
    "embed = embedding_layer(mask)\n",
    "l1 = lstm1(embed)\n",
    "drop1 = Dropout(0.1)(l1)\n",
    "maxim = GlobalMaxPooling1D()(drop1)\n",
    "att = SelfAttLayer(name='attention')(drop1)\n",
    "out = concatenate([maxim, att])\n",
    "HeadlineEncoder = Model(input_headline, maxim, name='HeadlineEncoder')\n",
    "\n",
    "# HeadlineEncoder.set_weights(layer_dict['SentenceEncoder'].get_weights())\n",
    "\n",
    "##############################\n",
    "\n",
    "# Define the document encoder #\n",
    "\n",
    "body_sentence = TimeDistributed(HeadlineEncoder)(input_body)\n",
    "body_g1 = right_branch_lstm1(body_sentence)\n",
    "body_g1 = Dropout(0.1)(body_g1)\n",
    "body_maxim = GlobalMaxPooling1D()(body_g1)\n",
    "body_att = SelfAttLayer()(body_g1)\n",
    "body_out = concatenate([body_maxim, body_att])\n",
    "DocumentEncoder = Model(input_body, body_maxim, name='DocumentEncoder')\n",
    "\n",
    "##############################\n",
    "\n",
    "# Combining both representations #\n",
    "\n",
    "headline_representation = HeadlineEncoder(input_headline)\n",
    "document_representation = DocumentEncoder(input_body)\n",
    "\n",
    "# Match between headline and first two sentences from body #\n",
    "\n",
    "two_sentences_representation = HeadlineEncoder(input_two)\n",
    "concat_two = concatenate([headline_representation, two_sentences_representation])\n",
    "mul_two = multiply([headline_representation, two_sentences_representation])\n",
    "dif_two = subtract([headline_representation, two_sentences_representation])\n",
    "final_merge_two = concatenate([concat_two, mul_two, dif_two, input_overlap_two, input_refuting_two, input_polarity_two, input_hand_two, \\\n",
    "                               input_sim_two, input_bleu_two, input_rouge_two, input_cider_two])\n",
    "drop3_two = Dropout(0.1)(final_merge_two)\n",
    "dense1_two = Dense(hidden_units*2, activation='relu')(drop3_two)\n",
    "# , weights=layer_dict['dense1'].get_weights()\n",
    "drop4_two = Dropout(0.1)(dense1_two)\n",
    "dense2_two = Dense(hidden_units, activation='relu')(drop4_two)\n",
    "# ,weights=layer_dict['dense2'].get_weights()\n",
    "match = Dropout(0.1)(dense2_two)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "concat = concatenate([headline_representation, document_representation])\n",
    "mul = multiply([headline_representation, document_representation])\n",
    "dif = subtract([headline_representation, document_representation])\n",
    "final_merge = concatenate([concat, mul, dif, input_overlap, input_refuting, input_polarity, input_hand, input_sim, input_bleu, input_rouge, input_cider])\n",
    "drop3 = Dropout(0.1)(final_merge)\n",
    "dense1 = Dense(hidden_units*2, activation='relu', name='dense1')(drop3)\n",
    "# , weights=layer_dict['dense1'].get_weights()\n",
    "drop4 = Dropout(0.1)(dense1)\n",
    "dense2 = Dense(hidden_units, activation='relu', name='dense2')(drop4)\n",
    "# , weights=layer_dict['dense2'].get_weights()\n",
    "drop5 = Dropout(0.1)(dense2)\n",
    "concat_final = concatenate([drop5,match,input_talos_count, input_talos_tfidfsim, input_talos_headline_svd, input_talos_body_svd, \\\n",
    "                     input_talos_svdsim, input_talos_headline_w2v, input_talos_body_w2v, input_talos_w2vsim, \\\n",
    "                     input_talos_headline_senti, input_talos_body_senti])\n",
    "drop6 = Dropout(0.1)(concat_final)\n",
    "dense3 = Dense(4, activation='softmax')(drop6)\n",
    "final_model = Model([input_headline, input_body,input_overlap, input_refuting, input_polarity, input_hand, \\\n",
    "                     input_sim, input_sim_two, input_bleu, input_rouge,input_cider, input_two, input_overlap_two, input_refuting_two, input_polarity_two, input_hand_two, \\\n",
    "                     input_bleu_two, input_rouge_two, input_cider_two, input_talos_count, input_talos_tfidfsim, input_talos_headline_svd, input_talos_body_svd, \\\n",
    "                     input_talos_svdsim, input_talos_headline_w2v, input_talos_body_w2v, input_talos_w2vsim, \\\n",
    "                     input_talos_headline_senti, input_talos_body_senti], dense3)\n",
    "#######################################################################################\n",
    "\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Articles \n",
    "\n",
    "keyss = ['Iran' , 'Olympic']\n",
    "def get_articles(keyss):\n",
    "  er = EventRegistry(apiKey = 'ENTER API KEY')\n",
    "  q = QueryArticlesIter(\n",
    "      keywords = QueryItems.AND(keyss),\n",
    "   dataType   = [\"news\", \"blog\"],\n",
    "       lang = 'eng',\n",
    "       startSourceRankPercentile = 0,\n",
    "      endSourceRankPercentile = 30)\n",
    "  # obtain at most 500 newest articles or blog posts\n",
    "  all_data=[]\n",
    "  for art in q.execQuery(er, sortBy = \"relevancy\", maxItems = 40):\n",
    "      all_data.append(art)\n",
    "  return(all_data)\n",
    "\n",
    "all_articles = get_articles(keyss)\n",
    "len(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = 'Iran’s Only Female Olympic Medalist Defects Over ‘Lies’ and ‘Injustice’'\n",
    "# bodyy = \"\"\"Elon Musk kicked off a Tuesday presentation at Tesla’s Shanghai plant by tearing off his blazer and dancing enthusiastically to a song by his girlfriend, pop star Grimes. The eccentric CEO and founder has a multibillion-dollar reason to be in such high spirits. Tesla stock has spiked since the company revealed last week that it delivered a record 112,000 vehicles in the fourth quarter, modestly exceeding the consensus estimate among analysts, and announced that its first full-scale production plant outside the U.S.—in Shanghai, China—is now delivering vehicles. Shares surged 13% in the week through January 9—boosting Musk’s net worth by $2.1 billion, to $28.8 billion. The 49-year-old billionaire is currently the 34th-richest person in the world. Musk owns nearly 22% of Tesla’s stock, which accounts for roughly half of his fortune. He also has an estimated $12.5 billion stake in SpaceX, his privately held aerospace company known for reusable rockets. Today In: Billionaires CHINA-AUTOMOBILE-MANUFACTURING-TESLA-US Musk dances to Grimes' \"Oblivion\" at Tesla's Shanghai plant.AFP VIA GETTY IMAGES On Tuesday, Tesla’s market cap hit $83 billion, making it the most valuable car company in American history (not accounting for inflation), besting Ford’s market value of $81 billion in 1999, according to data from Dow Jones. By Friday’s market close, Tesla’s market cap had crept all the way up to $85.8 billion. That is close behind the $87 billion combined market cap of GM and Ford. PROMOTED Tesla stock has doubled since October, but there have been road bumps along the way. In November, Musk’s net worth fell by $768 million in a single day after the botched unveiling of the Cybertruck sunk Tesla’s share price by 6%. And Wall Street analysts remain highly divided on the company. Out of a group of 36 analysts tracked by Bloomberg, 15 have issued sell ratings, 11 have issued holds and 10 have issued buys. Wedbush analyst Daniel Ives is confident in Tesla, highlighting growth potential in China. “We believe Tesla’s ability to ramp production and demand in the key China region during the course of 2020 will be a major swing factor on the stock,” Ives wrote in a research note. JPMorgan analyst Ryan Brinkman maintained his sell rating. Though he raised his price target, Brinkman noted that deliveries for the more expensive Models S and X (versus the Model 3) have fallen 30% year over year. “We remain underweight-rated on TSLA shares, however, on what we see as a lofty valuation (an $80 billion market cap after its recent run for a company about to report its tenth consecutive annual net loss) coupled with high investor expectations and high execution risk,” Brinkman wrote. CFRA analyst Garrett Nelson, meanwhile, downgraded his rating from hold to sell. “In our view, TSLA shares now appear fully valued after a meteoric run-up since bottoming at $177 a share last June and are not reflecting various near-term risks,” Nelson wrote. “With TSLA’s market cap now exceeding GM and Ford combined despite having only about 3% of joint vehicle sales volume, we think investors have given TSLA plenty of credit for future growth, raising execution risk.” Heavenly Bodies: Fashion & The Catholic Imagination Costume Institute Gala - Cocktails Musk and Canadian pop star Grimes attend the 2018 Met Gala.GETTY IMAGES FOR VOGUE It has been a busy week for Musk. His girlfriend, Grimes, has posted two pregnancy photos on Instagram. Musk has yet to formally confirm he is the father, but in response to a tweeted video of him dancing to Grimes’ song, “Oblivion,” he cheekily replied, “Haha spectrum af” with a baby emoji. He also commented on Grimes’ tweet of a pregnancy photo with “x is y.” If Tesla’s run-up continues, its share price may double again by the time Musk’s seventh child is born—adding billions to his already massive net worth. \"\"\"\n",
    "all_body = []\n",
    "for i in all_articles:\n",
    "  all_body.append(i['body'])\n",
    "\n",
    "all_head = []\n",
    "for i in range(0,len(all_body)):\n",
    "  all_head.append(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_head = all_head\n",
    "X2_body = all_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "overlapFeatures_fnc_test = np.array(word_overlap_features(X1_head, X2_body))\n",
    "refutingFeatures_fnc_test = np.array(refuting_features(X1_head, X2_body))\n",
    "polarityFeatures_fnc_test = np.array(polarity_features(X1_head, X2_body))\n",
    "handFeatures_fnc_test = np.array(hand_features(X1_head, X2_body))\n",
    "\n",
    "bleu_fnc_test = []\n",
    "for i in range(len(X1_head)):\n",
    "    split_doc = sent_tokenize(X1_head[i])\n",
    "    for j in range(len(split_doc)):\n",
    "        split_doc[j] = word_tokenize(clean_fnc(split_doc[j]))\n",
    "    bleu_fnc_test += [ sentence_bleu(split_doc, word_tokenize(X1_head[i]) ) ]\n",
    "bleu_fnc_test = np.array(bleu_fnc_test)\n",
    "\n",
    "rouge_fnc_test = []\n",
    "fails = 0\n",
    "for i in range(len(X1_head)):\n",
    "    rouge_values = []\n",
    "    try:\n",
    "        scores = rouge.get_scores(clean(X2_test[i]), clean(X1_head[i]))\n",
    "        rouge_values += [scores[0]['rouge-1']['f']]\n",
    "        rouge_values += [scores[0]['rouge-2']['f']]\n",
    "        rouge_values += [scores[0]['rouge-l']['f']]\n",
    "    except:\n",
    "        fails += 1\n",
    "        rouge_values = [0,0,0]\n",
    "    rouge_fnc_test += [rouge_values]\n",
    "print(\"ROUGE FNC TEST: {} fails\".format(fails))\n",
    "rouge_fnc_test = np.array(rouge_fnc_test)\n",
    "\n",
    "X2_test_two_sentences = []\n",
    "for document in X2_body:\n",
    "    sentences = sent_tokenize(document)\n",
    "    try:\n",
    "        X2_test_two_sentences += ['| ' + clean_fnc(sentences[0]) + ' ' + clean_fnc(sentences[1]) + ' |']\n",
    "    except:\n",
    "        X2_test_two_sentences += ['| ' + clean_fnc(sentences[0]) + ' |']\n",
    "  \n",
    "\n",
    "overlapFeatures_fnc_two_test = np.array(word_overlap_features(X1_head, X2_test_two_sentences))\n",
    "refutingFeatures_fnc_two_test = np.array(refuting_features(X1_head, X2_test_two_sentences))\n",
    "polarityFeatures_fnc_two_test = np.array(polarity_features(X1_head, X2_test_two_sentences))\n",
    "handFeatures_fnc_two_test = np.array(hand_features(X1_head, X2_test_two_sentences))\n",
    "\n",
    "\n",
    "bleu_two_sentences_test = []\n",
    "for i in range(len(X1_head)):\n",
    "    bleu_two_sentences_test += [ sentence_bleu(word_tokenize(clean_fnc(X2_test_two_sentences[i])), \\\n",
    "                                          word_tokenize(X1_head[i]) ) ]\n",
    "bleu_two_sentences_test = np.array(bleu_two_sentences_test)\n",
    "\n",
    "\n",
    "\n",
    "rouge_two_sentences_test = []\n",
    "for i in range(len(X1_head)):\n",
    "    rouge_values = []\n",
    "    scores = rouge.get_scores(clean(X2_test_two_sentences[i]), clean(X1_head[i]))\n",
    "    rouge_values += [scores[0]['rouge-1']['f']]\n",
    "    rouge_values += [scores[0]['rouge-2']['f']]\n",
    "    rouge_values += [scores[0]['rouge-l']['f']]\n",
    "    rouge_two_sentences_test += [rouge_values]\n",
    "rouge_two_sentences_test = np.array(rouge_two_sentences_test)\n",
    "\n",
    "X2_test_two_sentences = sequence.pad_sequences( tokenizer.texts_to_sequences( X2_test_two_sentences ) , maxlen=max_seq_len )\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X1_head = sequence.pad_sequences( tokenizer.texts_to_sequences( X1_head) , maxlen=max_seq_len ) # Mesma coisa para headlines de teste\n",
    "X1_head = np.asarray(X1_head)\n",
    "\n",
    "data_aux = np.zeros( ( len(X2_body) , max_seqs , max_seq_len ) ) # len(X2) = numero total de bodies do dataset, max_seq_len = 30, max_seqs = 15\n",
    "for i, sentences in enumerate(X2_body):\n",
    "    sentences = sent_tokenize( sentences )\n",
    "    sentences = list(map(lambda x: '| ' + clean_fnc(x) + ' |', sentences))\n",
    "    aux = [ ]\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < max_seqs: data_aux[i,j] = sequence.pad_sequences( tokenizer.texts_to_sequences( [ sent ] ) , maxlen=max_seq_len )[0]\n",
    "X2_body = np.asarray(data_aux)\n",
    "\n",
    "# final_model.load_weights(\"fnc-weights.h5\")\n",
    "final_model1 = load_model(\"fnc-weights-save.h5\")\n",
    "\n",
    "test_outputs = []\n",
    "test_predictions = []\n",
    "labels = ['unrelated', 'agree', 'disagree', 'discuss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = final_model1.predict([X1_head, \n",
    "                                  X2_body, overlapFeatures_fnc_test, refutingFeatures_fnc_test, polarityFeatures_fnc_test, handFeatures_fnc_test,  \\\n",
    "                                  cosFeatures_fnc_test, cosFeatures_two_test, bleu_fnc_test, rouge_fnc_test, cider_fnc_test, \\\n",
    "                                  X2_test_two_sentences, overlapFeatures_fnc_two_test, refutingFeatures_fnc_two_test, polarityFeatures_fnc_two_test, handFeatures_fnc_two_test, \\\n",
    "                                  bleu_two_sentences_test, rouge_two_sentences_test, cider_two_test, talos_counts_test, talos_tfidfsim_test, talos_svdHeadline_test, talos_svdBody_test, talos_svdsim_test, \\\n",
    "                                  talos_w2vHeadline_test, \n",
    "                               talos_w2vBody_test, \n",
    "                           talos_w2vsim_test, \n",
    "                           talos_sentiHeadline_test, \n",
    "                           talos_sentiBody_test])\n",
    "# aux = final_model1.predict([X1_head, X2_body])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "preds = []\n",
    "for prediction in aux:\n",
    "    pred = prediction.argmax()\n",
    "    if pred == 0:\n",
    "        one_hot = [1,0,0,0]\n",
    "    if pred == 1:\n",
    "        one_hot = [0,1,0,0]\n",
    "    if pred == 2:\n",
    "        one_hot = [0,0,1,0]\n",
    "    if pred == 3:\n",
    "        one_hot = [0,0,0,1]\n",
    "    preds += [one_hot]\n",
    "    test_predictions += [labels[prediction.argmax()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(test_predictions):\n",
    "  score = 0\n",
    "  count = 0 \n",
    "  for i in test_predictions:\n",
    "    if i == 'agree':\n",
    "      score += 1\n",
    "      \n",
    "    if i == 'disagree':\n",
    "      score-= 2\n",
    "    if i == 'discuss':\n",
    "      score -= 0\n",
    "       \n",
    "    count +=1\n",
    "  if score > 1:\n",
    "    print('REAL ', score)\n",
    "  else:\n",
    "    print('FAKE', score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ling = learn_classify.predict(head)\n",
    "print('*' * 15)\n",
    "\n",
    "print('Stance only')\n",
    "score(test_predictions)\n",
    "print('*' * 15)\n",
    "print('Linguistics Only', ling)\n",
    "print('*' * 15)\n",
    "\n",
    "hh = ling[2][0]\n",
    "hhh = ling[2][1] #Second Accuracy \n",
    "\n",
    "# if hh > hhh:\n",
    "#   meta = hh\n",
    "\n",
    "# else:\n",
    "#   meta = hhh\n",
    "\n",
    "if len(all_body) > 3 and len(keyss) > 1:\n",
    "      print('Stance Detection Comined - ')\n",
    "      score(test_predictions)\n",
    "\n",
    "elif hhh < 0.7 and len(all_body) < 10:\n",
    "     print('FAKE')\n",
    "\n",
    "else:\n",
    "  print('Linguisics Comined - ')\n",
    "  print(learn_classify.predict(bodyy))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
